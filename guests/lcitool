#!/usr/bin/env python3

# lcitool - libvirt CI guest management tool
# Copyright (C) 2017-2018  Andrea Bolognani <abologna@redhat.com>
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program. If not, see <https://www.gnu.org/licenses/>.

import argparse
import configparser
import distutils.spawn
import fnmatch
import json
import os
import platform
import random
import string
import subprocess
import sys
import tempfile
import textwrap
import yaml


class Util:

    @staticmethod
    def get_base():
        return os.path.dirname(os.path.abspath(__file__))

    @staticmethod
    def mksalt():
        alphabeth = string.ascii_letters + string.digits
        salt = "".join(random.choice(alphabeth) for x in range(0, 16))
        return "$6${}$".format(salt)

    @staticmethod
    def expand_pattern(pattern, source, name):
        if pattern is None:
            raise Exception("Missing {} list".format(name))

        if pattern == "all":
            pattern = "*"

        # This works correctly for single items as well as more complex
        # cases such as explicit lists, glob patterns and any combination
        # of the above
        matches = []
        for partial_pattern in pattern.split(","):

            partial_matches = []
            for item in source:
                if fnmatch.fnmatch(item, partial_pattern):
                    partial_matches.append(item)

            if not partial_matches:
                raise Exception("Invalid {} list '{}'".format(name, pattern))

            matches.extend(partial_matches)

        return sorted(set(matches))

    @staticmethod
    def get_native_arch():
        # Same canonicalization as libvirt virArchFromHost
        arch = platform.machine()
        if arch in ["i386", "i486", "i586"]:
            arch = "i686"
        if arch == "amd64":
            arch = "x86_64"
        return arch

    @staticmethod
    def native_arch_to_abi(native_arch):
        archmap = {
            "aarch64": "aarch64-linux-gnu",
            "armv6l": "arm-linux-gnueabi",
            "armv7l": "arm-linux-gnueabihf",
            "i686": "i686-linux-gnu",
            "mingw32": "i686-w64-mingw32",
            "mingw64": "x86_64-w64-mingw32",
            "mips": "mips-linux-gnu",
            "mipsel": "mipsel-linux-gnu",
            "mips64el": "mips64el-linux-gnuabi64",
            "ppc64le": "powerpc64le-linux-gnu",
            "s390x": "s390x-linux-gnu",
            "x86_64": "x86_64-linux-gnu",
        }
        if native_arch not in archmap:
            raise Exception("Unsupported architecture {}".format(native_arch))
        return archmap[native_arch]

    @staticmethod
    def native_arch_to_deb_arch(native_arch):
        archmap = {
            "aarch64": "arm64",
            "armv6l": "armel",
            "armv7l": "armhf",
            "i686": "i386",
            "mips": "mips",
            "mipsel": "mipsel",
            "mips64el": "mips64el",
            "ppc64le": "ppc64el",
            "s390x": "s390x",
            "x86_64": "amd64",
        }
        if native_arch not in archmap:
            raise Exception("Unsupported architecture {}".format(native_arch))
        return archmap[native_arch]


class Config:

    def __init__(self):

        # Load the template config containing the defaults first, this must
        # always succeed.
        # NOTE: we should load this from /usr/share once we start packaging
        # lcitool
        base = Util.get_base()
        with open(os.path.join(base, "config.yaml"), "r") as fp:
            self.values = yaml.safe_load(fp)

        user_config_path = self._get_config_file("config.yaml")
        if not os.path.exists(user_config_path):
            return

        try:
            with open(user_config_path, "r") as fp:
                user_config = yaml.safe_load(fp)
        except Exception as e:
            raise Exception("Invalid config.yaml file: {}".format(e))

        if user_config is None:
            raise Exception("Invalid config.yaml file")

        # delete user params we don't recognize
        self._remove_all_unknown_keys(user_config)

        # Override the default settings with user config
        self._update(user_config)

    @staticmethod
    def _get_config_file(name):
        try:
            config_dir = os.environ["XDG_CONFIG_HOME"]
        except KeyError:
            config_dir = os.path.join(os.environ["HOME"], ".config")
        config_dir = os.path.join(config_dir, "lcitool")

        return os.path.join(config_dir, name)

    @staticmethod
    def _remove_unknown_keys(_dict, known_keys):
        keys = list(_dict.keys())

        for k in keys:
            if k not in known_keys:
                del _dict[k]

    def _remove_all_unknown_keys(self, config):
        # remove keys we don't recognize
        self._remove_unknown_keys(config, self.values.keys())
        for section in self.values.keys():
            if section in config:
                self._remove_unknown_keys(config[section], self.values[section].keys())

    def _validate_section(self, section, mandatory_keys):
        # check that the mandatory keys are present and non-empty
        for key in mandatory_keys:
            if self.values.get(section).get(key) is None:
                raise Exception(("Missing or empty value for mandatory key "
                                 "'{}.{}'").format(section, key))

        # check that all keys have values assigned and of the right type
        for key in self.values[section].keys():

            # mandatory keys were already checked, so this covers optional keys
            if self.values[section][key] is None:
                raise Exception(
                    "Missing value for '{}.{}'".format(section, key)
                )

            if not isinstance(self.values[section][key], (str, int)):
                raise Exception(
                    "Invalid type for key '{}.{}'".format(section, key)
                )

    # Validate that parameters needed for VM install are present
    def validate_vm_settings(self):
        self._validate_section("install", ["root_password"])

        flavor = self.values["install"].get("flavor")
        if flavor not in ["test", "gitlab"]:
            raise Exception(
                "Invalid value '{}' for 'install.flavor'".format(flavor)
            )

        if flavor == "gitlab":
            self._validate_section("gitlab", ["runner_secret"])

    def _update(self, values):
        for section in self.values.keys():
            if section in values:
                self.values[section].update(values[section])


class Inventory:

    def __init__(self):
        base = Util.get_base()
        ansible_cfg_path = os.path.join(base, "ansible.cfg")

        try:
            parser = configparser.ConfigParser()
            parser.read(ansible_cfg_path)
            inventory_path = parser.get("defaults", "inventory")
        except Exception as ex:
            raise Exception(
                "Can't read inventory location in ansible.cfg: {}".format(ex))

        inventory_path = os.path.join(base, inventory_path)

        self._facts = {}
        try:
            # We can only deal with trivial inventories, but that's
            # all we need right now and we can expand support further
            # later on if necessary
            with open(inventory_path, "r") as infile:
                for line in infile:
                    host = line.strip()
                    self._facts[host] = {}
        except Exception as ex:
            raise Exception(
                "Missing or invalid inventory ({}): {}".format(
                    inventory_path, ex
                )
            )

        for host in self._facts:
            try:
                self._facts[host] = self._read_all_facts(host)
                self._facts[host]["inventory_hostname"] = host
            except Exception as ex:
                raise Exception("Can't load facts for '{}': {}".format(
                    host, ex))

    @staticmethod
    def _add_facts_from_file(facts, yaml_path):
        with open(yaml_path, "r") as infile:
            some_facts = yaml.safe_load(infile)
            for fact in some_facts:
                facts[fact] = some_facts[fact]

    def _read_all_facts(self, host):
        base = Util.get_base()

        sources = [
            os.path.join(base, "group_vars", "all"),
            os.path.join(base, "host_vars", host),
        ]

        facts = {}

        # We load from group_vars/ first and host_vars/ second, sorting
        # files alphabetically; doing so should result in our view of
        # the facts matching Ansible's
        for source in sources:
            for item in sorted(os.listdir(source)):
                yaml_path = os.path.join(source, item)
                if not os.path.isfile(yaml_path):
                    continue
                if not yaml_path.endswith(".yml"):
                    continue
                self._add_facts_from_file(facts, yaml_path)

        return facts

    def expand_pattern(self, pattern):
        return Util.expand_pattern(pattern, self._facts, "host")

    def get_facts(self, host):
        return self._facts[host]


class Projects:

    def __init__(self):
        base = Util.get_base()

        mappings_path = os.path.join(base, "vars", "mappings.yml")

        try:
            with open(mappings_path, "r") as infile:
                mappings = yaml.safe_load(infile)
                self._mappings = mappings["mappings"]
                self._pypi_mappings = mappings["pypi_mappings"]
                self._cpan_mappings = mappings["cpan_mappings"]
        except Exception as ex:
            raise Exception("Can't load mappings: {}".format(ex))

        source = os.path.join(base, "vars", "projects")

        self._packages = {}
        for item in os.listdir(source):
            yaml_path = os.path.join(source, item)
            if not os.path.isfile(yaml_path):
                continue
            if not yaml_path.endswith(".yml"):
                continue

            project = os.path.splitext(item)[0]

            try:
                with open(yaml_path, "r") as infile:
                    packages = yaml.safe_load(infile)
                    self._packages[project] = packages["packages"]
            except Exception as ex:
                raise Exception(
                    "Can't load packages for '{}': {}".format(project, ex))

    def expand_pattern(self, pattern):
        projects = Util.expand_pattern(pattern, self._packages, "project")

        # Some projects are internal implementation details and should
        # not be exposed to the user
        for project in ["base", "unwanted", "cloud-init"]:
            if project in projects:
                projects.remove(project)

        return projects

    def get_mappings(self):
        return self._mappings

    def get_pypi_mappings(self):
        return self._pypi_mappings

    def get_cpan_mappings(self):
        return self._cpan_mappings

    def get_packages(self, project):
        return self._packages[project]


class CommandLine:

    def __init__(self):
        self._parser = argparse.ArgumentParser(
            conflict_handler="resolve",
            description="libvirt CI guest management tool",
        )

        self._parser.add_argument("--debug", action="store_true",
                                  help="display debugging information")

        subparsers = self._parser.add_subparsers(metavar="ACTION")
        subparsers.required = True

        def add_hosts_arg(parser):
            parser.add_argument(
                "hosts",
                help="list of hosts to act on (accepts globs)",
            )

        def add_projects_arg(parser):
            parser.add_argument(
                "projects",
                help="list of projects to consider (accepts globs)",
            )

        def add_gitrev_arg(parser):
            parser.add_argument(
                "-g", "--git-revision",
                help="git revision to build (remote/branch)",
            )

        def add_cross_arch_arg(parser):
            parser.add_argument(
                "-x", "--cross-arch",
                help="target architecture for cross compiler",
            )

        def add_wait_arg(parser):
            parser.add_argument(
                "-w", "--wait",
                help="wait for installation to complete",
                default=False,
                action="store_true",
            )

        installparser = subparsers.add_parser(
            "install", help="perform unattended host installation")
        installparser.set_defaults(func=Application._action_install)

        add_hosts_arg(installparser)
        add_wait_arg(installparser)

        updateparser = subparsers.add_parser(
            "update", help="prepare hosts and keep them updated")
        updateparser.set_defaults(func=Application._action_update)

        add_hosts_arg(updateparser)
        add_projects_arg(updateparser)
        add_gitrev_arg(updateparser)

        buildparser = subparsers.add_parser(
            "build", help="build projects on hosts")
        buildparser.set_defaults(func=Application._action_build)

        add_hosts_arg(buildparser)
        add_projects_arg(buildparser)
        add_gitrev_arg(buildparser)

        hostsparser = subparsers.add_parser(
            "hosts", help="list all known hosts")
        hostsparser.set_defaults(func=Application._action_hosts)

        projectsparser = subparsers.add_parser(
            "projects", help="list all known projects")
        projectsparser.set_defaults(func=Application._action_projects)

        dockerfileparser = subparsers.add_parser(
            "dockerfile", help="generate Dockerfile (doesn't access the host)")
        dockerfileparser.set_defaults(func=Application._action_dockerfile)

        add_hosts_arg(dockerfileparser)
        add_projects_arg(dockerfileparser)
        add_cross_arch_arg(dockerfileparser)

    def parse(self):
        return self._parser.parse_args()


class Application:

    def __init__(self):
        self._config = Config()
        self._inventory = Inventory()
        self._projects = Projects()

        self._native_arch = Util.get_native_arch()

    def _execute_playbook(self, playbook, hosts, projects, git_revision):
        base = Util.get_base()
        config = self._config

        config.validate_vm_settings()

        ansible_hosts = ",".join(self._inventory.expand_pattern(hosts))
        selected_projects = self._projects.expand_pattern(projects)

        if git_revision is not None:
            tokens = git_revision.split("/")
            if len(tokens) < 2:
                raise Exception(
                    "Missing or invalid git revision '{}'".format(git_revision)
                )
            git_remote = tokens[0]
            git_branch = "/".join(tokens[1:])
        else:
            git_remote = "default"
            git_branch = "master"

        tempdir = tempfile.TemporaryDirectory(prefix="lcitool")

        ansible_cfg_path = os.path.join(base, "ansible.cfg")
        playbook_base = os.path.join(base, "playbooks", playbook)
        playbook_path = os.path.join(playbook_base, "main.yml")
        extra_vars_path = os.path.join(tempdir.name, "extra_vars.json")

        extra_vars = config.values
        extra_vars.update({
            "base": base,
            "playbook_base": playbook_base,
            "selected_projects": selected_projects,
            "git_remote": git_remote,
            "git_branch": git_branch,
        })

        with open(extra_vars_path, "w") as fp:
            json.dump(extra_vars, fp)

        ansible_playbook = distutils.spawn.find_executable("ansible-playbook")
        if ansible_playbook is None:
            raise Exception("Cannot find ansible-playbook in $PATH")

        cmd = [
            ansible_playbook,
            "--limit", ansible_hosts,
            "--extra-vars", "@" + extra_vars_path,
        ]

        cmd.append(playbook_path)

        # We need to point Ansible to the correct configuration file,
        # and for some reason this has to be done using the environment
        # rather than through the command line
        os.environ["ANSIBLE_CONFIG"] = ansible_cfg_path

        try:
            subprocess.check_call(cmd)
        except Exception as ex:
            raise Exception(
                "Failed to run {} on '{}': {}".format(playbook, hosts, ex))
        finally:
            tempdir.cleanup()

    def _action_hosts(self, args):
        for host in self._inventory.expand_pattern("all"):
            print(host)

    def _action_projects(self, args):
        for project in self._projects.expand_pattern("all"):
            print(project)

    def _action_install(self, args):
        base = Util.get_base()
        config = self._config

        config.validate_vm_settings()

        for host in self._inventory.expand_pattern(args.hosts):
            facts = self._inventory.get_facts(host)

            # Both memory size and disk size are stored as GiB in the
            # inventory, but virt-install expects the disk size in GiB
            # and the memory size in *MiB*, so perform conversion here
            memory_arg = str(config.values["install"]["memory_size"] * 1024)

            vcpus_arg = str(config.values["install"]["vcpus"])

            disk_arg = "size={},pool={},bus=virtio".format(
                config.values["install"]["disk_size"],
                config.values["install"]["storage_pool"],
            )
            network_arg = "network={},model=virtio".format(
                config.values["install"]["network"],
            )

            # Different operating systems require different configuration
            # files for unattended installation to work, but some operating
            # systems simply don't support unattended installation at all
            if facts["os"]["name"] in ["Debian", "Ubuntu"]:
                install_config = "preseed.cfg"
            elif facts["os"]["name"] in ["CentOS", "Fedora"]:
                install_config = "kickstart.cfg"
            elif facts["os"]["name"] == "OpenSUSE":
                install_config = "autoinst.xml"
            else:
                raise Exception(
                    "Host {} doesn't support installation".format(host)
                )

            # Unattended install scripts are being generated on the fly, based
            # on the templates present in guests/configs/
            unattended_options = {
                "install.url": facts["install"]["url"],
            }

            initrd_template = os.path.join(base, "configs", install_config)
            with open(initrd_template, 'r') as template:
                content = template.read()
                for option in unattended_options:
                    content = content.replace(
                        "{{ " + option + " }}",
                        unattended_options[option],
                    )

            tempdir = tempfile.TemporaryDirectory(prefix="lcitool")
            initrd_inject = os.path.join(tempdir.name, install_config)

            with open(initrd_inject, "w") as inject:
                inject.write(content)

            # preseed files must use a well-known name to be picked up by
            # d-i; for kickstart files, we can use whatever name we please
            # but we need to point anaconda in the right direction through
            # the 'ks' kernel parameter. We can use 'ks' unconditionally
            # for simplicity's sake, because distributions that don't use
            # kickstart for unattended installation will simply ignore it.
            # We do the same with the 'install' argument in order to
            # workaround a bug which causes old virt-install versions to
            # not pass the URL correctly when installing openSUSE guests
            extra_arg = "console=ttyS0 ks=file:/{} install={}".format(
                install_config,
                facts["install"]["url"],
            )

            virt_install = distutils.spawn.find_executable("virt-install")
            if virt_install is None:
                raise Exception("Cannot find virt-install in $PATH")

            cmd = [
                virt_install,
                "--name", host,
                "--location", facts["install"]["url"],
                "--virt-type", config.values["install"]["virt_type"],
                "--arch", config.values["install"]["arch"],
                "--machine", config.values["install"]["machine"],
                "--cpu", config.values["install"]["cpu_model"],
                "--vcpus", vcpus_arg,
                "--memory", memory_arg,
                "--disk", disk_arg,
                "--network", network_arg,
                "--graphics", "none",
                "--console", "pty",
                "--sound", "none",
                "--rng", "device=/dev/urandom,model=virtio",
                "--initrd-inject", initrd_inject,
                "--extra-args", extra_arg,
            ]

            if not args.wait:
                cmd.append("--noautoconsole")

            try:
                subprocess.check_call(cmd)
            except Exception as ex:
                raise Exception("Failed to install '{}': {}".format(host, ex))
            finally:
                tempdir.cleanup()

    def _action_update(self, args):
        self._execute_playbook("update", args.hosts, args.projects,
                               args.git_revision)

    def _action_build(self, args):
        self._execute_playbook("build", args.hosts, args.projects,
                               args.git_revision)

    def _get_openvz_repo(self):
        base = Util.get_base()
        repofile = os.path.join(base, "playbooks", "update", "templates", "openvz.repo.j2")
        with open(repofile, "r") as r:
            return r.read().rstrip()

    def _get_openvz_key(self):
        base = Util.get_base()
        keyfile = os.path.join(base, "playbooks", "update", "templates", "openvz.key")
        with open(keyfile, "r") as r:
            return r.read().rstrip()

    def _dockerfile_build_varmap(self, facts,
                                 mappings,
                                 pypi_mappings,
                                 cpan_mappings,
                                 projects,
                                 cross_arch):
        pkgs = {}
        cross_pkgs = {}
        pypi_pkgs = {}
        cpan_pkgs = {}
        base_keys = [
            "default",
            facts["packaging"]["format"],
            facts["os"]["name"],
            facts["os"]["name"] + facts["os"]["version"],
        ]
        cross_keys = []
        cross_policy_keys = []
        if cross_arch:
            keys = base_keys
            if facts["packaging"]["format"] == "deb":
                # For Debian-based distros, the name of the foreign package
                # is usually the same as the native package, but there might
                # be architecture-specific overrides, so we have to look both
                # at the neutral keys and at the specific ones
                cross_keys = base_keys + [cross_arch + "-" + k for k in base_keys]
            elif facts["packaging"]["format"] == "rpm":
                # For RPM-based distros, the name of the foreign package is
                # usually very different from the native one, so we should
                # only look at the keys that are specific to cross-building
                # because otherwise we'd also pick up a bunch of native
                # packages we don't actually need
                cross_keys = [cross_arch + "-" + k for k in base_keys]
            cross_policy_keys = ["cross-policy-" + k for k in base_keys]
        else:
            keys = base_keys + [self._native_arch + "-" + k for k in base_keys]

        # We need to add the base project manually here: the standard
        # machinery hides it because it's an implementation detail
        for project in projects + ["base"]:
            for package in self._projects.get_packages(project):
                cross_policy = "native"

                if (package not in mappings and
                    package not in pypi_mappings and
                    package not in cpan_mappings):
                    raise Exception(
                        "No mapping defined for {}".format(package)
                    )

                if package in mappings:
                    for key in cross_policy_keys:
                        if key in mappings[package]:
                            cross_policy = mappings[package][key]

                    if cross_policy not in ["native", "foreign", "skip"]:
                        raise Exception(
                            "Unexpected cross arch policy {} for {}".format
                            (cross_policy, package))

                    if cross_arch and cross_policy == "foreign":
                        for key in cross_keys:
                            if key in mappings[package]:
                                pkgs[package] = mappings[package][key]
                    else:
                        for key in keys:
                            if key in mappings[package]:
                                pkgs[package] = mappings[package][key]

                if package in pypi_mappings:
                    if "default" in pypi_mappings[package]:
                        pypi_pkgs[package] = pypi_mappings[package]["default"]

                if package in cpan_mappings:
                    if "default" in cpan_mappings[package]:
                        cpan_pkgs[package] = cpan_mappings[package]["default"]

                if package in pkgs and pkgs[package] is None:
                    del pkgs[package]
                if package in pypi_pkgs and pypi_pkgs[package] is None:
                    del pypi_pkgs[package]
                if package in cpan_pkgs and cpan_pkgs[package] is None:
                    del cpan_pkgs[package]
                if package in pypi_pkgs and package in pkgs:
                    del pypi_pkgs[package]
                if package in cpan_pkgs and package in pkgs:
                    del cpan_pkgs[package]

                if (package not in pkgs and
                    package not in pypi_pkgs and
                    package not in cpan_pkgs):
                    continue

                if package in pkgs and cross_policy == "foreign":
                    cross_pkgs[package] = pkgs[package]
                if package in pkgs and cross_policy in ["skip", "foreign"]:
                    del pkgs[package]

        varmap = {
            "packaging_command": facts["packaging"]["command"],
            "paths_cc": facts["paths"]["cc"],
            "paths_ccache": facts["paths"]["ccache"],
            "paths_make": facts["paths"]["make"],
            "paths_ninja": facts["paths"]["ninja"],
            "paths_python": facts["paths"]["python"],
        }

        varmap["pkgs"] = sorted(set(pkgs.values()))

        if cross_arch:
            varmap["cross_arch"] = cross_arch
            varmap["cross_abi"] = Util.native_arch_to_abi(cross_arch)

            if facts["packaging"]["format"] == "deb":
                # For Debian-based distros, the name of the foreign package
                # is obtained by appending the foreign architecture (in
                # Debian format) to the name of the native package
                cross_arch_deb = Util.native_arch_to_deb_arch(cross_arch)
                cross_pkgs = [p + ":" + cross_arch_deb for p in set(cross_pkgs.values())]
                cross_pkgs.append("gcc-" + varmap["cross_abi"])
                varmap["cross_arch_deb"] = cross_arch_deb
                varmap["cross_pkgs"] = sorted(cross_pkgs)
            elif facts["packaging"]["format"] == "rpm":
                # For RPM-based distros, all mappings have already been
                # resolved and we just need to add the cross-compiler
                cross_pkgs["gcc"] = cross_arch + "-gcc"
                varmap["cross_pkgs"] = sorted(set(cross_pkgs.values()))

        if pypi_pkgs:
            varmap["pypi_pkgs"] = sorted(set(pypi_pkgs.values()))
        if cpan_pkgs:
            varmap["cpan_pkgs"] = sorted(set(cpan_pkgs.values()))

        return varmap

    def _dockerfile_format(self, facts, cross_arch, varmap):
        pkg_align = " \\\n" + (" " * len("RUN " + facts["packaging"]["command"] + " "))
        pypi_pkg_align = " \\\n" + (" " * len("RUN pip3 "))
        cpan_pkg_align = " \\\n" + (" " * len("RUN cpanm "))

        varmap["pkgs"] = pkg_align[1:] + pkg_align.join(varmap["pkgs"])

        if "cross_pkgs" in varmap:
            varmap["cross_pkgs"] = pkg_align[1:] + pkg_align.join(varmap["cross_pkgs"])
        if "pypi_pkgs" in varmap:
            varmap["pypi_pkgs"] = pypi_pkg_align[1:] + pypi_pkg_align.join(varmap["pypi_pkgs"])
        if "cpan_pkgs" in varmap:
            varmap["cpan_pkgs"] = cpan_pkg_align[1:] + cpan_pkg_align.join(varmap["cpan_pkgs"])

        print("FROM {}".format(facts["docker"]["base"]))

        commands = []

        if facts["packaging"]["format"] == "deb":
            commands.extend([
                "export DEBIAN_FRONTEND=noninteractive",
                "{packaging_command} update",
                "{packaging_command} dist-upgrade -y",
                "{packaging_command} install --no-install-recommends -y {pkgs}",
                "{packaging_command} autoremove -y",
                "{packaging_command} autoclean -y",
                "sed -Ei 's,^# (en_US\\.UTF-8 .*)$,\\1,' /etc/locale.gen",
                "dpkg-reconfigure locales",
            ])
        elif facts["packaging"]["format"] == "rpm":
            # Rawhide needs this because the keys used to sign packages are
            # cycled from time to time
            if facts["os"]["name"] == "Fedora" and facts["os"]["version"] == "Rawhide":
                commands.extend([
                    "{packaging_command} update -y --nogpgcheck fedora-gpg-keys",
                ])

            if facts["os"]["name"] == "CentOS":
                # For the Stream release we need to install the Stream
                # repositories
                if facts["os"]["version"] == "Stream":
                    commands.append(
                        "{packaging_command} install -y centos-release-stream"
                    )

                # Starting with CentOS 8, most -devel packages are shipped in
                # the PowerTools repository, which is not enabled by default
                if facts["os"]["version"] != "7":
                    powertools = "PowerTools"

                    # for the Stream release, we want the Stream-Powertools
                    # version of the repository
                    if facts["os"]["version"] == "Stream":
                        powertools = "Stream-PowerTools"

                    commands.extend([
                        "{packaging_command} install 'dnf-command(config-manager)' -y",
                        "{packaging_command} config-manager --set-enabled -y " + powertools,
                    ])

                # VZ development packages are only available for CentOS/RHEL-7
                # right now and need a 3rd party repository enabled
                if facts["os"]["version"] == "7":
                    repo = self._get_openvz_repo()
                    varmap["vzrepo"] = "\\n\\\n".join(repo.split("\n"))
                    key = self._get_openvz_key()
                    varmap["vzkey"] = "\\n\\\n".join(key.split("\n"))

                    commands.extend([
                        "echo -e '{vzrepo}' > /etc/yum.repos.d/openvz.repo",
                        "echo -e '{vzkey}' > /etc/pki/rpm-gpg/RPM-GPG-KEY-OpenVZ",
                        "rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-OpenVZ",
                    ])

                # Some of the packages we need are not part of CentOS proper
                # and are only available through EPEL
                commands.extend([
                    "{packaging_command} install -y epel-release",
                ])

            commands.extend([
                "{packaging_command} update -y",
                "{packaging_command} install -y {pkgs}",
            ])

            # openSUSE doesn't seem to have a convenient way to remove all
            # unnecessary packages, but CentOS and Fedora do
            if facts["os"]["name"] == "OpenSUSE":
                commands.extend([
                    "{packaging_command} clean --all",
                ])
            else:
                commands.extend([
                    "{packaging_command} autoremove -y",
                    "{packaging_command} clean all -y",
                ])

        commands.extend([
            "mkdir -p /usr/libexec/ccache-wrappers",
        ])

        if cross_arch:
            commands.extend([
                "ln -s {paths_ccache} /usr/libexec/ccache-wrappers/{cross_abi}-cc",
                "ln -s {paths_ccache} /usr/libexec/ccache-wrappers/{cross_abi}-$(basename {paths_cc})",
            ])
        else:
            commands.extend([
                "ln -s {paths_ccache} /usr/libexec/ccache-wrappers/cc",
                "ln -s {paths_ccache} /usr/libexec/ccache-wrappers/$(basename {paths_cc})",
            ])

        script = "\nRUN " + (" && \\\n    ".join(commands)) + "\n"
        sys.stdout.write(script.format(**varmap))

        if cross_arch:
            cross_commands = []

            # Intentionally a separate RUN command from the above
            # so that the common packages of all cross-built images
            # share a Docker image layer.
            if facts["packaging"]["format"] == "deb":
                cross_commands.extend([
                    "export DEBIAN_FRONTEND=noninteractive",
                    "dpkg --add-architecture {cross_arch_deb}",
                    "{packaging_command} update",
                    "{packaging_command} dist-upgrade -y",
                    "{packaging_command} install --no-install-recommends -y dpkg-dev",
                    "{packaging_command} install --no-install-recommends -y {cross_pkgs}",
                    "{packaging_command} autoremove -y",
                    "{packaging_command} autoclean -y",
                ])
            elif facts["packaging"]["format"] == "rpm":
                cross_commands.extend([
                    "{packaging_command} install -y {cross_pkgs}",
                    "{packaging_command} clean all -y",
                ])

            cross_script = "\nRUN " + (" && \\\n    ".join(cross_commands)) + "\n"
            sys.stdout.write(cross_script.format(**varmap))

        if "pypi_pkgs" in varmap:
            sys.stdout.write(textwrap.dedent("""
                RUN pip3 install {pypi_pkgs}
            """).format(**varmap))

        if "cpan_pkgs" in varmap:
            sys.stdout.write(textwrap.dedent("""
                RUN cpanm --notest {cpan_pkgs}
            """).format(**varmap))

        sys.stdout.write(textwrap.dedent("""
            ENV LANG "en_US.UTF-8"

            ENV MAKE "{paths_make}"
            ENV NINJA "{paths_ninja}"
            ENV PYTHON "{paths_python}"

            ENV CCACHE_WRAPPERSDIR "/usr/libexec/ccache-wrappers"
        """).format(**varmap))

        if cross_arch:
            cross_vars = [
                "ENV ABI \"{cross_abi}\"",
                "ENV CONFIGURE_OPTS \"--host={cross_abi}\"",
            ]

            if cross_arch.startswith("mingw"):
                cross_vars.append(
                    "ENV MESON_OPTS \"--cross-file=/usr/share/mingw/toolchain-{cross_arch}.meson\""
                )

            cross_env = "\n" + "\n".join(cross_vars) + "\n"
            sys.stdout.write(cross_env.format(**varmap))

    def _action_dockerfile(self, args):
        mappings = self._projects.get_mappings()
        pypi_mappings = self._projects.get_pypi_mappings()
        cpan_mappings = self._projects.get_cpan_mappings()

        hosts = self._inventory.expand_pattern(args.hosts)
        if len(hosts) > 1:
            raise Exception("Can't generate Dockerfile for multiple hosts")
        host = hosts[0]

        facts = self._inventory.get_facts(host)
        cross_arch = args.cross_arch

        if facts["packaging"]["format"] not in ["deb", "rpm"]:
            raise Exception("Host {} doesn't support Dockerfiles".format(host))
        if cross_arch:
            if facts["os"]["name"] not in ["Debian", "Fedora"]:
                raise Exception("Cannot cross compile on {}".format(
                    facts["os"]["name"],
                ))
            if facts["os"]["name"] == "Debian" and cross_arch.startswith("mingw"):
                raise Exception(
                    "Cannot cross compile for {} on {}".format(
                        cross_arch,
                        facts["os"]["name"],
                    )
                )
            if facts["os"]["name"] == "Fedora" and not cross_arch.startswith("mingw"):
                raise Exception(
                    "Cannot cross compile for {} on {}".format(
                        cross_arch,
                        facts["os"]["name"],
                    )
                )
            if cross_arch == self._native_arch:
                raise Exception("Cross arch {} should differ from native {}".
                                format(cross_arch, self._native_arch))

        projects = self._projects.expand_pattern(args.projects)
        for project in projects:
            if project.rfind("+mingw") >= 0:
                raise Exception("Obsolete syntax, please use --cross-arch")

        varmap = self._dockerfile_build_varmap(facts,
                                               mappings,
                                               pypi_mappings,
                                               cpan_mappings,
                                               projects,
                                               cross_arch)
        self._dockerfile_format(facts, cross_arch, varmap)

    def run(self, args):
        args.func(self, args)


if __name__ == "__main__":
    args = CommandLine().parse()

    if args.debug:
        Application().run(args)
    else:
        try:
            Application().run(args)
        except Exception as err:
            sys.stderr.write("{}: {}\n".format(sys.argv[0], err))
            sys.exit(1)
