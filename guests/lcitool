#!/usr/bin/env python3

# lcitool - libvirt CI guest management tool
# Copyright (C) 2017-2018  Andrea Bolognani <abologna@redhat.com>
#
# This program is free software; you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the
# Free Software Foundation; either version 2 of the License, or (at your
# option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General
# Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program. If not, see <https://www.gnu.org/licenses/>.

import argparse
import configparser
import distutils.spawn
import fnmatch
import json
import os
import platform
import random
import string
import subprocess
import sys
import tempfile
import textwrap
import yaml


class Util:

    @staticmethod
    def get_base():
        return os.path.dirname(os.path.abspath(__file__))

    @staticmethod
    def mksalt():
        alphabeth = string.ascii_letters + string.digits
        salt = "".join(random.choice(alphabeth) for x in range(0, 16))
        return "$6${}$".format(salt)

    @staticmethod
    def expand_pattern(pattern, source, name):
        if pattern is None:
            raise Exception("Missing {} list".format(name))

        if pattern == "all":
            pattern = "*"

        # This works correctly for single items as well as more complex
        # cases such as explicit lists, glob patterns and any combination
        # of the above
        matches = []
        for partial_pattern in pattern.split(","):

            partial_matches = []
            for item in source:
                if fnmatch.fnmatch(item, partial_pattern):
                    partial_matches += [item]

            if not partial_matches:
                raise Exception("Invalid {} list '{}'".format(name, pattern))

            matches += partial_matches

        return sorted(set(matches))

    @staticmethod
    def get_native_arch():
        # Same canonicalization as libvirt virArchFromHost
        arch = platform.machine()
        if arch in ["i386", "i486", "i586"]:
            arch = "i686"
        if arch == "amd64":
            arch = "x86_64"
        return arch

    @staticmethod
    def native_arch_to_abi(native_arch):
        archmap = {
            "aarch64": "aarch64-linux-gnu",
            "armv6l": "arm-linux-gnueabi",
            "armv7l": "arm-linux-gnueabihf",
            "i686": "i686-linux-gnu",
            "mingw32": "i686-w64-mingw32",
            "mingw64": "x86_64-w64-mingw32",
            "mips": "mips-linux-gnu",
            "mipsel": "mipsel-linux-gnu",
            "mips64el": "mips64el-linux-gnuabi64",
            "ppc64le": "powerpc64le-linux-gnu",
            "s390x": "s390x-linux-gnu",
            "x86_64": "x86_64-linux-gnu",
        }
        if native_arch not in archmap:
            raise Exception("Unsupported architecture {}".format(native_arch))
        return archmap[native_arch]

    @staticmethod
    def native_arch_to_lib(native_arch):
        arch = Util.native_arch_to_abi(native_arch)
        if arch == "i686-linux-gnu":
            arch = "i386-linux-gnu"
        return arch

    @staticmethod
    def native_arch_to_deb_arch(native_arch):
        archmap = {
            "aarch64": "arm64",
            "armv6l": "armel",
            "armv7l": "armhf",
            "i686": "i386",
            "mips": "mips",
            "mipsel": "mipsel",
            "mips64el": "mips64el",
            "ppc64le": "ppc64el",
            "s390x": "s390x",
            "x86_64": "amd64",
        }
        if native_arch not in archmap:
            raise Exception("Unsupported architecture {}".format(native_arch))
        return archmap[native_arch]


class Config:

    def __init__(self):

        # Load the template config containing the defaults first, this must
        # always succeed.
        # NOTE: we should load this from /usr/share once we start packaging
        # lcitool
        base = Util.get_base()
        with open(os.path.join(base, "config.yaml"), "r") as fp:
            self.values = yaml.safe_load(fp)

        try:
            with open(self._get_config_file("config.yaml"), "r") as fp:
                user_config = yaml.safe_load(fp)
        except Exception as e:
            raise Exception("Missing or invalid config.yaml file: {}".format(e))

        if user_config is None:
            raise Exception("Missing or invalid config.yaml file")

        # Validate the user provided config and use it to override the default
        # settings
        self._validate(user_config)
        self._update(user_config)

    @staticmethod
    def _get_config_file(name):
        try:
            config_dir = os.environ["XDG_CONFIG_HOME"]
        except KeyError:
            config_dir = os.path.join(os.environ["HOME"], ".config")
        config_dir = os.path.join(config_dir, "lcitool")

        # Create the directory if it doesn't already exist
        if not os.path.exists(config_dir):
            try:
                os.mkdir(config_dir)
            except Exception as ex:
                raise Exception(
                    "Can't create configuration directory ({}): {}".format(
                        config_dir, ex,
                    )
                )

        return os.path.join(config_dir, name)

    @staticmethod
    def _remove_unknown_keys(_dict, known_keys):
        keys = list(_dict.keys())

        for k in keys:
            if k not in known_keys:
                del _dict[k]

    def _validate_section(self, config, section, mandatory_keys):
        # remove keys we don't recognize
        self._remove_unknown_keys(config[section], self.values[section].keys())

        # check that the mandatory keys are present and non-empty
        for key in mandatory_keys:
            if config.get(section).get(key) is None:
                raise Exception(("Missing or empty value for mandatory key"
                                 "'{}.{}'").format(section, key))

        # check that all keys have values assigned and of the right type
        for key in config[section].keys():

            # mandatory keys were already checked, so this covers optional keys
            if config[section][key] is None:
                raise Exception(
                    "Missing value for '{}.{}'".format(section, key)
                )

            if not isinstance(config[section][key], (str, int)):
                raise Exception(
                    "Invalid type for key '{}.{}'".format(section, key)
                )

    def _validate(self, config):
        # delete sections we don't recognize
        self._remove_unknown_keys(config, self.values.keys())

        if "install" not in config:
            raise Exception("Missing mandatory section 'install'")

        self._validate_section(config, "install", ["root_password"])

        # we only need this for the gitlab check below, if 'flavor' is missing
        # that's okay, we'll provide a default later
        flavor = config["install"].get("flavor")
        if flavor is not None and flavor not in ["test", "jenkins", "gitlab"]:
            raise Exception(
                "Invalid value '{}' for 'install.flavor'".format(flavor)
            )

        if flavor == "gitlab":
            self._validate_section(config, "gitlab", ["runner_secret"])

    def _update(self, values):
        self.values["install"].update(values["install"])

        if values.get("gitlab") is not None:
            self.values["gitlab"].update(values["gitlab"])

    def get_vault_password_file(self):
        vault_pass_file = None

        # The vault password is only needed for the jenkins flavor, but in
        # that case we want to make sure there's *something* in there
        if self.values["install"]["flavor"] == "jenkins":
            vault_pass_file = self._get_config_file("vault-password")

            try:
                with open(vault_pass_file, "r") as infile:
                    if not infile.readline().strip():
                        raise ValueError
            except Exception as ex:
                raise Exception(
                    "Missing or invalid vault password file ({}): {}".format(
                        vault_pass_file, ex
                    )
                )

        return vault_pass_file

    def get_gitlab_runner_token_file(self):
        if self.values["install"]["flavor"] != "gitlab":
            return None

        gitlab_runner_token_file = self._get_config_file("gitlab-runner-token")

        try:
            with open(gitlab_runner_token_file, "r") as infile:
                if not infile.readline().strip():
                    raise ValueError
        except Exception as ex:
            raise Exception(
                "Missing or invalid GitLab runner token file ({}): {}".format(
                    gitlab_runner_token_file, ex
                )
            )

        return gitlab_runner_token_file

    def get_gitlab_url_file(self):
        if self.values["install"]["flavor"] != "gitlab":
            return None

        gitlab_url_file = self._get_config_file("gitlab-url")

        try:
            with open(gitlab_url_file, "r") as infile:
                if not infile.readline().strip():
                    raise ValueError
        except Exception as ex:
            raise Exception(
                "Missing or invalid GitLab url file ({}): {}".format(
                    gitlab_url_file, ex
                )
            )

        return gitlab_url_file


class Inventory:

    def __init__(self):
        base = Util.get_base()
        ansible_cfg_path = os.path.join(base, "ansible.cfg")

        try:
            parser = configparser.ConfigParser()
            parser.read(ansible_cfg_path)
            inventory_path = parser.get("defaults", "inventory")
        except Exception as ex:
            raise Exception(
                "Can't read inventory location in ansible.cfg: {}".format(ex))

        inventory_path = os.path.join(base, inventory_path)

        self._facts = {}
        try:
            # We can only deal with trivial inventories, but that's
            # all we need right now and we can expand support further
            # later on if necessary
            with open(inventory_path, "r") as infile:
                for line in infile:
                    host = line.strip()
                    self._facts[host] = {}
        except Exception as ex:
            raise Exception(
                "Missing or invalid inventory ({}): {}".format(
                    inventory_path, ex
                )
            )

        for host in self._facts:
            try:
                self._facts[host] = self._read_all_facts(host)
                self._facts[host]["inventory_hostname"] = host
            except Exception as ex:
                raise Exception("Can't load facts for '{}': {}".format(
                    host, ex))

    @staticmethod
    def _add_facts_from_file(facts, yaml_path):
        with open(yaml_path, "r") as infile:
            some_facts = yaml.safe_load(infile)
            for fact in some_facts:
                facts[fact] = some_facts[fact]

    def _read_all_facts(self, host):
        base = Util.get_base()

        sources = [
            os.path.join(base, "group_vars", "all"),
            os.path.join(base, "host_vars", host),
        ]

        facts = {}

        # We load from group_vars/ first and host_vars/ second, sorting
        # files alphabetically; doing so should result in our view of
        # the facts matching Ansible's
        for source in sources:
            for item in sorted(os.listdir(source)):
                yaml_path = os.path.join(source, item)
                if not os.path.isfile(yaml_path):
                    continue
                if not yaml_path.endswith(".yml"):
                    continue
                self._add_facts_from_file(facts, yaml_path)

        return facts

    def expand_pattern(self, pattern):
        return Util.expand_pattern(pattern, self._facts, "host")

    def get_facts(self, host):
        return self._facts[host]


class Projects:

    def __init__(self):
        base = Util.get_base()

        mappings_path = os.path.join(base, "vars", "mappings.yml")

        try:
            with open(mappings_path, "r") as infile:
                mappings = yaml.safe_load(infile)
                self._mappings = mappings["mappings"]
                self._pip_mappings = mappings["pip_mappings"]
        except Exception as ex:
            raise Exception("Can't load mappings: {}".format(ex))

        source = os.path.join(base, "vars", "projects")

        self._packages = {}
        for item in os.listdir(source):
            yaml_path = os.path.join(source, item)
            if not os.path.isfile(yaml_path):
                continue
            if not yaml_path.endswith(".yml"):
                continue

            project = os.path.splitext(item)[0]

            try:
                with open(yaml_path, "r") as infile:
                    packages = yaml.safe_load(infile)
                    self._packages[project] = packages["packages"]
            except Exception as ex:
                raise Exception(
                    "Can't load packages for '{}': {}".format(project, ex))

    def expand_pattern(self, pattern):
        projects = Util.expand_pattern(pattern, self._packages, "project")

        # Some projects are internal implementation details and should
        # not be exposed to the user
        for project in ["base", "blacklist", "jenkins"]:
            if project in projects:
                projects.remove(project)

        return projects

    def get_mappings(self):
        return self._mappings

    def get_pip_mappings(self):
        return self._pip_mappings

    def get_packages(self, project):
        return self._packages[project]


class CommandLine:

    def __init__(self):
        self._parser = argparse.ArgumentParser(
            conflict_handler="resolve",
            description="libvirt CI guest management tool",
        )

        self._parser.add_argument("--debug", action="store_true",
                                  help="display debugging information")

        subparsers = self._parser.add_subparsers(metavar="ACTION")
        subparsers.required = True

        def add_hosts_arg(parser):
            parser.add_argument(
                "hosts",
                help="list of hosts to act on (accepts globs)",
            )

        def add_projects_arg(parser):
            parser.add_argument(
                "projects",
                help="list of projects to consider (accepts globs)",
            )

        def add_gitrev_arg(parser):
            parser.add_argument(
                "-g", "--git-revision",
                help="git revision to build (remote/branch)",
            )

        def add_cross_arch_arg(parser):
            parser.add_argument(
                "-x", "--cross-arch",
                help="target architecture for cross compiler",
            )

        def add_wait_arg(parser):
            parser.add_argument(
                "-w", "--wait",
                help="wait for installation to complete",
                default=False,
                action="store_true",
            )

        installparser = subparsers.add_parser(
            "install", help="perform unattended host installation")
        installparser.set_defaults(func=Application._action_install)

        add_hosts_arg(installparser)
        add_wait_arg(installparser)

        updateparser = subparsers.add_parser(
            "update", help="prepare hosts and keep them updated")
        updateparser.set_defaults(func=Application._action_update)

        add_hosts_arg(updateparser)
        add_projects_arg(updateparser)
        add_gitrev_arg(updateparser)

        buildparser = subparsers.add_parser(
            "build", help="build projects on hosts")
        buildparser.set_defaults(func=Application._action_build)

        add_hosts_arg(buildparser)
        add_projects_arg(buildparser)
        add_gitrev_arg(buildparser)

        hostsparser = subparsers.add_parser(
            "hosts", help="list all known hosts")
        hostsparser.set_defaults(func=Application._action_hosts)

        projectsparser = subparsers.add_parser(
            "projects", help="list all known projects")
        projectsparser.set_defaults(func=Application._action_projects)

        dockerfileparser = subparsers.add_parser(
            "dockerfile", help="generate Dockerfile (doesn't access the host)")
        dockerfileparser.set_defaults(func=Application._action_dockerfile)

        add_hosts_arg(dockerfileparser)
        add_projects_arg(dockerfileparser)
        add_cross_arch_arg(dockerfileparser)

    def parse(self):
        return self._parser.parse_args()


class Application:

    def __init__(self):
        self._config = Config()
        self._inventory = Inventory()
        self._projects = Projects()

        self._native_arch = Util.get_native_arch()

    def _execute_playbook(self, playbook, hosts, projects, git_revision):
        base = Util.get_base()

        vault_pass_file = self._config.get_vault_password_file()
        gitlab_url_file = self._config.get_gitlab_url_file()
        gitlab_runner_token_file = self._config.get_gitlab_runner_token_file()

        ansible_hosts = ",".join(self._inventory.expand_pattern(hosts))
        selected_projects = self._projects.expand_pattern(projects)

        if git_revision is not None:
            tokens = git_revision.split("/")
            if len(tokens) < 2:
                raise Exception(
                    "Missing or invalid git revision '{}'".format(git_revision)
                )
            git_remote = tokens[0]
            git_branch = "/".join(tokens[1:])
        else:
            git_remote = "default"
            git_branch = "master"

        tempdir = tempfile.TemporaryDirectory(prefix="lcitool")

        ansible_cfg_path = os.path.join(base, "ansible.cfg")
        playbook_base = os.path.join(base, "playbooks", playbook)
        playbook_path = os.path.join(playbook_base, "main.yml")
        extra_vars_path = os.path.join(tempdir.name, "extra_vars.json")

        extra_vars = self._config.values
        extra_vars.update({
            "base": base,
            "playbook_base": playbook_base,
            "selected_projects": selected_projects,
            "git_remote": git_remote,
            "git_branch": git_branch,
        })

        with open(extra_vars_path, "w") as fp:
            json.dump(self._config.values, fp)

        ansible_playbook = distutils.spawn.find_executable("ansible-playbook")
        if ansible_playbook is None:
            raise Exception("Cannot find ansible-playbook in $PATH")

        cmd = [
            ansible_playbook,
            "--limit", ansible_hosts,
            "--extra-vars", "@" + extra_vars_path,
        ]

        # Provide the vault password if available
        if vault_pass_file is not None:
            cmd += ["--vault-password-file", vault_pass_file]

        cmd += [playbook_path]

        # We need to point Ansible to the correct configuration file,
        # and for some reason this has to be done using the environment
        # rather than through the command line
        os.environ["ANSIBLE_CONFIG"] = ansible_cfg_path

        try:
            subprocess.check_call(cmd)
        except Exception as ex:
            raise Exception(
                "Failed to run {} on '{}': {}".format(playbook, hosts, ex))
        finally:
            tempdir.cleanup()

    def _action_hosts(self, args):
        for host in self._inventory.expand_pattern("all"):
            print(host)

    def _action_projects(self, args):
        for project in self._projects.expand_pattern("all"):
            print(project)

    def _action_install(self, args):
        base = Util.get_base()

        for host in self._inventory.expand_pattern(args.hosts):
            facts = self._inventory.get_facts(host)

            # Both memory size and disk size are stored as GiB in the
            # inventory, but virt-install expects the disk size in GiB
            # and the memory size in *MiB*, so perform conversion here
            memory_arg = str(int(facts["install_memory_size"]) * 1024)

            vcpus_arg = str(facts["install_vcpus"])

            disk_arg = "size={},pool={},bus=virtio".format(
                facts["install_disk_size"],
                facts["install_storage_pool"],
            )
            network_arg = "network={},model=virtio".format(
                facts["install_network"],
            )

            # Different operating systems require different configuration
            # files for unattended installation to work, but some operating
            # systems simply don't support unattended installation at all
            if facts["os"]["name"] in ["Debian", "Ubuntu"]:
                install_config = "preseed.cfg"
            elif facts["os"]["name"] in ["CentOS", "Fedora"]:
                install_config = "kickstart.cfg"
            elif facts["os"]["name"] == "OpenSUSE":
                install_config = "autoinst.xml"
            else:
                raise Exception(
                    "Host {} doesn't support installation".format(host)
                )

            # Unattended install scripts are being generated on the fly, based
            # on the templates present in guests/configs/
            unattended_options = [
                "install_url",
            ]

            initrd_template = os.path.join(base, "configs", install_config)
            with open(initrd_template, 'r') as template:
                content = template.read()
                for option in unattended_options:
                    content = content.replace(
                        "{{ " + option + " }}",
                        facts[option]
                    )

            tempdir = tempfile.TemporaryDirectory(prefix="lcitool")
            initrd_inject = os.path.join(tempdir.name, install_config)

            with open(initrd_inject, "w") as inject:
                inject.write(content)

            # preseed files must use a well-known name to be picked up by
            # d-i; for kickstart files, we can use whatever name we please
            # but we need to point anaconda in the right direction through
            # the 'ks' kernel parameter. We can use 'ks' unconditionally
            # for simplicity's sake, because distributions that don't use
            # kickstart for unattended installation will simply ignore it.
            # We do the same with the 'install' argument in order to
            # workaround a bug which causes old virt-install versions to
            # not pass the URL correctly when installing openSUSE guests
            extra_arg = "console=ttyS0 ks=file:/{} install={}".format(
                install_config,
                facts["install_url"],
            )

            virt_install = distutils.spawn.find_executable("virt-install")
            if virt_install is None:
                raise Exception("Cannot find virt-install in $PATH")

            cmd = [
                virt_install,
                "--name", host,
                "--location", facts["install_url"],
                "--virt-type", facts["install_virt_type"],
                "--arch", facts["install_arch"],
                "--machine", facts["install_machine"],
                "--cpu", facts["install_cpu_model"],
                "--vcpus", vcpus_arg,
                "--memory", memory_arg,
                "--disk", disk_arg,
                "--network", network_arg,
                "--graphics", "none",
                "--console", "pty",
                "--sound", "none",
                "--rng", "device=/dev/urandom,model=virtio",
                "--initrd-inject", initrd_inject,
                "--extra-args", extra_arg,
            ]

            if not args.wait:
                cmd.append("--noautoconsole")

            # Only configure autostart for the guest for the jenkins flavor
            if self._config.values["install"]["flavor"] == "jenkins":
                cmd += ["--autostart"]

            try:
                subprocess.check_call(cmd)
            except Exception as ex:
                raise Exception("Failed to install '{}': {}".format(host, ex))
            finally:
                tempdir.cleanup()

    def _action_update(self, args):
        self._execute_playbook("update", args.hosts, args.projects,
                               args.git_revision)

    def _action_build(self, args):
        self._execute_playbook("build", args.hosts, args.projects,
                               args.git_revision)

    def _get_openvz_repo(self):
        base = Util.get_base()
        repofile = os.path.join(base, "playbooks", "update", "templates", "openvz.repo.j2")
        with open(repofile, "r") as r:
            return r.read().rstrip()

    def _get_openvz_key(self):
        base = Util.get_base()
        keyfile = os.path.join(base, "playbooks", "update", "templates", "openvz.key")
        with open(keyfile, "r") as r:
            return r.read().rstrip()

    def _dockerfile_build_varmap(self, facts, mappings, pip_mappings, projects, cross_arch):
        if facts["packaging"]["format"] == "deb":
            varmap = self._dockerfile_build_varmap_deb(facts, mappings, pip_mappings, projects, cross_arch)
        if facts["packaging"]["format"] == "rpm":
            varmap = self._dockerfile_build_varmap_rpm(facts, mappings, pip_mappings, projects, cross_arch)

        varmap["packaging_command"] = facts["packaging"]["command"]
        varmap["paths_cc"] = facts["paths"]["cc"]
        varmap["paths_ccache"] = facts["paths"]["ccache"]
        varmap["paths_make"] = facts["paths"]["make"]
        varmap["paths_ninja"] = facts["paths"]["ninja"]
        varmap["paths_python"] = facts["paths"]["python"]

        if cross_arch:
            varmap["cross_abi"] = Util.native_arch_to_abi(cross_arch)

        return varmap

    def _dockerfile_build_varmap_deb(self, facts, mappings, pip_mappings, projects, cross_arch):
        pkgs = {}
        cross_pkgs = {}
        pip_pkgs = {}
        base_keys = [
            "default",
            facts["packaging"]["format"],
            facts["os"]["name"],
            facts["os"]["name"] + facts["os"]["version"],
        ]
        cross_keys = []
        if cross_arch:
            keys = base_keys + [cross_arch + "-" + k for k in base_keys]
            cross_keys = ["cross-policy-" + k for k in base_keys]
        else:
            keys = base_keys + [self._native_arch + "-" + k for k in base_keys]

        # We need to add the base project manually here: the standard
        # machinery hides it because it's an implementation detail
        for project in projects + ["base"]:
            for package in self._projects.get_packages(project):
                cross_policy = "native"
                for key in cross_keys:
                    if key in mappings[package]:
                        cross_policy = mappings[package][key]
                if cross_policy not in ["native", "foreign", "skip"]:
                    raise Exception(
                        "Unexpected cross arch policy {} for {}".format
                        (cross_policy, package))

                for key in keys:
                    if key in mappings[package]:
                        pkgs[package] = mappings[package][key]
                    if package in pip_mappings and key in pip_mappings[package]:
                        pip_pkgs[package] = pip_mappings[package][key]

                if package not in pkgs:
                    continue
                if package in pip_pkgs and pkgs[package] is not None:
                    del pip_pkgs[package]
                if cross_policy == "foreign" and pkgs[package] is not None:
                    cross_pkgs[package] = pkgs[package]
                if pkgs[package] is None or cross_policy in ["skip", "foreign"]:
                    del pkgs[package]

        pkg_align = " \\\n" + (" " * len("RUN " + facts["packaging"]["command"] + " "))
        pip_pkg_align = " \\\n" + (" " * len("RUN pip3 "))

        varmap = {}
        varmap["pkgs"] = pkg_align[1:] + pkg_align.join(sorted(set(pkgs.values())))

        if cross_arch:
            deb_arch = Util.native_arch_to_deb_arch(cross_arch)
            abi = Util.native_arch_to_abi(cross_arch)
            lib = Util.native_arch_to_lib(cross_arch)
            gcc = "gcc-" + abi
            varmap["cross_arch"] = deb_arch
            pkg_names = [p + ":" + deb_arch for p in cross_pkgs.values()]
            pkg_names.append(gcc)
            varmap["cross_pkgs"] = pkg_align[1:] + pkg_align.join(sorted(set(pkg_names)))
            varmap["cross_lib"] = lib

        if pip_pkgs:
            varmap["pip_pkgs"] = pip_pkg_align[1:] + pip_pkg_align.join(sorted(set(pip_pkgs.values())))

        return varmap

    def _dockerfile_build_varmap_rpm(self, facts, mappings, pip_mappings, projects, cross_arch):
        pkgs = {}
        cross_pkgs = {}
        pip_pkgs = {}
        keys = [
            "default",
            facts["packaging"]["format"],
            facts["os"]["name"],
            facts["os"]["name"] + facts["os"]["version"],
        ]

        # We need to add the base project manually here: the standard
        # machinery hides it because it's an implementation detail
        for project in projects + ["base"]:
            for package in self._projects.get_packages(project):
                for key in keys:
                    if key in mappings[package]:
                        pkgs[package] = mappings[package][key]
                    if package in pip_mappings and key in pip_mappings[package]:
                        pip_pkgs[package] = pip_mappings[package][key]

                if package not in pkgs:
                    continue
                if package in pip_pkgs and pkgs[package] is not None:
                    del pip_pkgs[package]
                if pkgs[package] is None:
                    del pkgs[package]

        if cross_arch:
            cross_projects = []
            for project in projects:
                cross_project = project + "+" + cross_arch
                if cross_project in facts["projects"]:
                    cross_projects.extend([cross_project])

            for project in cross_projects:
                for package in self._projects.get_packages(project):
                    for key in keys:
                        if key in mappings[package]:
                            cross_pkgs[package] = mappings[package][key]

                    if package not in cross_pkgs:
                        continue
                    if cross_pkgs[package] is None:
                        del cross_pkgs[package]

        pkg_align = " \\\n" + (" " * len("RUN " + facts["packaging"]["command"] + " "))
        pip_pkg_align = " \\\n" + (" " * len("RUN pip3 "))

        varmap = {}
        varmap["pkgs"] = pkg_align[1:] + pkg_align.join(sorted(set(pkgs.values())))

        if cross_arch:
            varmap["cross_pkgs"] = pkg_align[1:] + pkg_align.join(sorted(set(cross_pkgs.values())))

        if pip_pkgs:
            varmap["pip_pkgs"] = pip_pkg_align[1:] + pip_pkg_align.join(sorted(set(pip_pkgs.values())))

        return varmap

    def _dockerfile_format(self, facts, cross_arch, varmap):
        print("FROM {}".format(facts["docker_base"]))

        commands = []

        if facts["packaging"]["format"] == "deb":
            commands.extend([
                "export DEBIAN_FRONTEND=noninteractive",
                "{packaging_command} update",
                "{packaging_command} dist-upgrade -y",
                "{packaging_command} install --no-install-recommends -y {pkgs}",
                "{packaging_command} autoremove -y",
                "{packaging_command} autoclean -y",
                "sed -Ei 's,^# (en_US\\.UTF-8 .*)$,\\1,' /etc/locale.gen",
                "dpkg-reconfigure locales",
            ])
        elif facts["packaging"]["format"] == "rpm":
            # Rawhide needs this because the keys used to sign packages are
            # cycled from time to time
            if facts["os"]["name"] == "Fedora" and facts["os"]["version"] == "Rawhide":
                commands.extend([
                    "{packaging_command} update -y --nogpgcheck fedora-gpg-keys"
                ])

            if facts["os"]["name"] == "CentOS":
                # Starting with CentOS 8, most -devel packages are shipped in
                # the PowerTools repository, which is not enabled by default
                if facts["os"]["version"] != "7":
                    commands.extend([
                        "{packaging_command} install 'dnf-command(config-manager)' -y",
                        "{packaging_command} config-manager --set-enabled PowerTools -y",
                    ])

                # VZ development packages are only available for CentOS/RHEL-7
                # right now and need a 3rd party repository enabled
                if facts["os"]["version"] == "7":
                    repo = self._get_openvz_repo()
                    varmap["vzrepo"] = "\\n\\\n".join(repo.split("\n"))
                    key = self._get_openvz_key()
                    varmap["vzkey"] = "\\n\\\n".join(key.split("\n"))

                    commands.extend([
                        "echo -e '{vzrepo}' > /etc/yum.repos.d/openvz.repo",
                        "echo -e '{vzkey}' > /etc/pki/rpm-gpg/RPM-GPG-KEY-OpenVZ",
                        "rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-OpenVZ"
                    ])

                # Some of the packages we need are not part of CentOS proper
                # and are only available through EPEL
                commands.extend([
                    "{packaging_command} install -y epel-release",
                ])

            commands.extend([
                "{packaging_command} update -y",
                "{packaging_command} install -y {pkgs}",
            ])

            # openSUSE doesn't seem to have a convenient way to remove all
            # unnecessary packages, but CentOS and Fedora do
            if facts["os"]["name"] == "OpenSUSE":
                commands.extend([
                    "{packaging_command} clean --all",
                ])
            else:
                commands.extend([
                    "{packaging_command} autoremove -y",
                    "{packaging_command} clean all -y",
                ])

        commands.extend([
            "mkdir -p /usr/libexec/ccache-wrappers",
        ])

        if cross_arch:
            commands.extend([
                "ln -s {paths_ccache} /usr/libexec/ccache-wrappers/{cross_abi}-cc",
                "ln -s {paths_ccache} /usr/libexec/ccache-wrappers/{cross_abi}-$(basename {paths_cc})",
            ])
        else:
            commands.extend([
                "ln -s {paths_ccache} /usr/libexec/ccache-wrappers/cc",
                "ln -s {paths_ccache} /usr/libexec/ccache-wrappers/$(basename {paths_cc})",
            ])

        script = "\nRUN " + (" && \\\n    ".join(commands)) + "\n"
        sys.stdout.write(script.format(**varmap))

        if cross_arch:
            cross_commands = []

            # Intentionally a separate RUN command from the above
            # so that the common packages of all cross-built images
            # share a Docker image layer.
            if facts["packaging"]["format"] == "deb":
                cross_commands.extend([
                    "export DEBIAN_FRONTEND=noninteractive",
                    "dpkg --add-architecture {cross_arch}",
                    "{packaging_command} update",
                    "{packaging_command} dist-upgrade -y",
                    "{packaging_command} install --no-install-recommends -y dpkg-dev",
                    "{packaging_command} install --no-install-recommends -y {cross_pkgs}",
                    "{packaging_command} autoremove -y",
                    "{packaging_command} autoclean -y",
                ])
            elif facts["packaging"]["format"] == "rpm":
                cross_commands.extend([
                    "{packaging_command} install -y {cross_pkgs}",
                    "{packaging_command} clean all -y",
                ])

            cross_script = "\nRUN " + (" && \\\n    ".join(cross_commands)) + "\n"
            sys.stdout.write(cross_script.format(**varmap))

        if "pip_pkgs" in varmap:
            sys.stdout.write(textwrap.dedent("""
                RUN pip3 install {pip_pkgs}
            """).format(**varmap))

        sys.stdout.write(textwrap.dedent("""
            ENV LANG "en_US.UTF-8"

            ENV MAKE "{paths_make}"
            ENV NINJA "{paths_ninja}"
            ENV PYTHON "{paths_python}"

            ENV CCACHE_WRAPPERSDIR "/usr/libexec/ccache-wrappers"
        """).format(**varmap))

        if cross_arch:
            sys.stdout.write(textwrap.dedent("""
                ENV ABI "{cross_abi}"
                ENV CONFIGURE_OPTS "--host={cross_abi}"
            """).format(**varmap))

    def _action_dockerfile(self, args):
        mappings = self._projects.get_mappings()
        pip_mappings = self._projects.get_pip_mappings()

        hosts = self._inventory.expand_pattern(args.hosts)
        if len(hosts) > 1:
            raise Exception("Can't generate Dockerfile for multiple hosts")
        host = hosts[0]

        facts = self._inventory.get_facts(host)
        cross_arch = args.cross_arch

        if facts["packaging"]["format"] not in ["deb", "rpm"]:
            raise Exception("Host {} doesn't support Dockerfiles".format(host))
        if cross_arch:
            if facts["os"]["name"] not in ["Debian", "Fedora"]:
                raise Exception("Cannot cross compile on {}".format(
                    facts["os"]["name"],
                ))
            if facts["os"]["name"] == "Debian" and cross_arch.startswith("mingw"):
                raise Exception(
                    "Cannot cross compile for {} on {}".format(
                        cross_arch,
                        facts["os"]["name"],
                    )
                )
            if facts["os"]["name"] == "Fedora" and not cross_arch.startswith("mingw"):
                raise Exception(
                    "Cannot cross compile for {} on {}".format(
                        cross_arch,
                        facts["os"]["name"],
                    )
                )
            if cross_arch == self._native_arch:
                raise Exception("Cross arch {} should differ from native {}".
                                format(cross_arch, self._native_arch))

        projects = self._projects.expand_pattern(args.projects)
        for project in projects:
            if project.rfind("+mingw") >= 0:
                raise Exception("Obsolete syntax, please use --cross-arch")

        varmap = self._dockerfile_build_varmap(facts, mappings, pip_mappings, projects, cross_arch)
        self._dockerfile_format(facts, cross_arch, varmap)

    def run(self, args):
        args.func(self, args)


if __name__ == "__main__":
    args = CommandLine().parse()

    if args.debug:
        Application().run(args)
    else:
        try:
            Application().run(args)
        except Exception as err:
            sys.stderr.write("{}: {}\n".format(sys.argv[0], err))
            sys.exit(1)
